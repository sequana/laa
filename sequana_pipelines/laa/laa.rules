#
#  Copyright (c) 2016-2021 Sequana Dev Team (https://sequana.readthedocs.io)
#
#  The full license is in the LICENSE file, distributed with this software.
#
#  Website:       https://github.com/sequana/sequana
#  Documentation: http://sequana.readthedocs.io
#  Contributors:  https://github.com/sequana/sequana/graphs/contributors
##############################################################################
# standard modules
import glob
import os
import shutil
import subprocess

import sequana
from sequana_pipetools import snaketools as sm


# ========================================================= The main config file
#
configfile: "config.yaml"


# ================================================== The sequana pipeline manager
#
manager = sm.PipelineManager("laa", config)

# set reference alias
reference = manager.config["reference_file"]

if len(manager.ff) == 0:
    logger.error("No files found.")
    sys.exit(0)

# Check the reference validity
assert reference.endswith('.fa') or reference.endswith('.fasta'), \
    "Reference must be a FASTA file ending in .fa or .fasta"


do_snpeff = config['snpeff']['do']
do_kraken = config['kraken']['do']
do_freebayes = config["freebayes"]["do"]
do_split_multiallelic = config["freebayes"]["split_multiallelic"]
do_freebayes_vcf_filter =  config["freebayes_vcf_filter"]["do"]


# identify the pattern

data_type = config['data_type_choice']


if data_type.endswith(".fastq") or data_type.endswith(".fastq.gz"):
    ccs2fastq = False
elif data_type.endswith(".bam"):
    ccs2fastq = True
else:
    raise ValueError("Input file type must be one of lima.ccs.fastq, lima.ccs.bam, subreads.bam")

#manager.samples = {tag:fl for tag, fl in zip(samples, manager.ff.realpaths)}


expected_output = [
    expand("{sample}/images/coverage.png", sample=manager.samples),
    "images/plot_ccs_histo.png",
    ".sequana/rulegraph.svg",
    "multiqc/multiqc_report.html",
    expand("{sample}/remapping/mapping.sorted.bam",sample=manager.samples),
    "outputs/mapping_consensus/mapping.sorted.bam",
    "outputs/raxml/RAxML_bipartitions.T3333",
    ]

if do_freebayes_vcf_filter:
    expected_output += expand("{sample}/report_variant/variant_calling.html", sample=manager.samples)

if config['itol']['do']:
    __phylogeny__output = "outputs/dendogram.png"
    expected_output.append(__phylogeny__output)


# Do kraken analysis ?
if do_kraken:
   expected_output.append("images/proportion_kraken.png")
   expected_output += expand("{sample}/taxonomy/summary.html", sample=manager.samples)


if do_snpeff:
    assert config['annotation_file'] and config["annotation_file"].endswith('.gbk'), "genbank must end in .gbk"

    __copy_genbank__output = "reference/" + os.path.basename(config['annotation_file'])
    rule copy_genbank:
        input: config["annotation_file"]
        output: __copy_genbank__output
        shell:
            """cp {input} {output}"""
    expected_output.append(__copy_genbank__output)


# MUST BE ONE OF THE FIRST RULE ?
rule pipeline:
    input: expected_output


laa = False
if laa is True:
    # Looks like the input is not a CCS file but a raw bam files after lima
    # 6 mins on BC25 of project 812 so we use protected
    rule laa:
        input: PATH + "lima_output.lbc{sample}--lbc{sample}.bam"
        output: protected("bc{sample}/amplicon_analysis_summary.csv"),
        params:
            directory="bc{sample}"
        threads: 2
        container:
            config["apptainers"]["laa"]
        shell:
            """
            cd {params.directory}
            laa --noPhasing --minLength 1000 --maxReads 2400  --numThreads={threads} {input}
            """


bam2ccs = False
if bam2ccs:
    rule bam2ccs:
        input: BAMPATH + "lima_output.lbc{sample}--lbc{sample}.bam"
        output: "bc{sample}/lima_output.lbc{sample}--lbc{sample}.ccs.bam"
        params:
            directory="bc{sample}"
        threads: 4
        container:
            config['apptainers']['ccs']
        shell:
            """
            ccs {input} {output} --numThreads {threads}
            """


if ccs2fastq:
    rule ccs2fastq:
        input: manager.getrawdata()
        output: "fastq/{sample}/data.fastq"
        container:
            config['apptainers']['samtools']
        shell:
            """
            samtools fastq {input} > {output}
            """


def get_input_fastq(wildcards):
    if ccs2fastq:
        return "fastq/{wildcards.sample}/data.fastq"
    else:
        return manager.samples[wildcards.sample]


def get_input_kraken(wildcards):
    if ccs2fastq:
        return "fastq/{wildcards.sample}/data.fastq"
    else:
        return manager.samples[wildcards.sample]


rule fastq_stats:
    input: get_input_fastq
    output: "{sample}/sequana_laa_{sample}.json"
    run:
        import json
        from sequana import FastQ
        f = FastQ(input[0])
        N = len(f)
        data = {"sample": output[0].split("/", 1)[0], "ccs_reads": N}
        with open(output[0], "w") as fout:
            json.dump(data, fout, indent=True, sort_keys=True)


reference_file  = config["reference_file"]
annotation_file = config["annotation_file"]
new_reference = f"reference/{os.path.basename(reference_file)}"


# ========================================================= snpeff
# Add locus in FASTA file for snpEff
if config["snpeff"]["do"]:
    #__snpeff_add_locus_in_fasta__input_fasta = reference
    new_reference = "reference/{0}".format(os.path.basename(reference))

    rule snpeff_add_locus_in_fasta:
        input:
            config["reference_file"],
            config["annotation_file"]
        output:
            new_reference
        params:
            options=config["snpeff"]["build_options"]
        log:
            "logs/snpeff_add_locus_in_fasta.log"
        container:
            config['apptainers']['sequana_tools']
        wrapper:
            f"{manager.wrappers}/wrappers/snpeff_add_locus_in_fasta"
    reference = new_reference
# Copy the reference index if it exists
elif not os.path.isfile(reference_file + ".fai"):
    rule copy_reference:
        input:
            src=reference_file
        output:
            src=new_reference
        shell:
            """
            cp {input.src} {output.src}
            """
else:
    new_reference = reference_file



rule mapping:
    # input data is fastq files.
    # If input is fastq, we can use the manager.rawdata variable that is
    # a dictionary with {sample} wildcards, otherwise a simple wildcard, which
    # is the output of ccs2fastq rule
    input:
        fastq=get_input_fastq,
        reference=reference
    output:
        sorted_bam = "{sample}/mapping/mapping.sorted.bam",
        bam = temp("{sample}/mapping/mapping.bam"),
        sam = temp("{sample}/mapping/mapping.sam"),
    params:
        reference = reference
    log:
        "{sample}/mapping/mapping.log"
    threads:
        4
    container:
        config['apptainers']['sequana_tools']
    shell:
        """
        minimap2 -x map-pb {input.reference} {input.fastq} -t {threads} -a 1> {output.sam} 2>{log}
        samtools view -bh {output.sam} > {output.bam}
        bamtools sort -in {output.bam} -out {output.sorted_bam}
        """


rule samtools_stats:
    input: "{sample}/mapping/mapping.sorted.bam"
    output: "{sample}/mapping/samtools_{sample}.txt"
    container:
        config['apptainers']['samtools']
    shell:
        "samtools stats -in {input} > {output}"


rule bamtools_stats:
    input: "{sample}/mapping/mapping.sorted.bam"
    output: "{sample}/mapping/sequana_bamtools_stats_{sample}.txt"
    container:
        config['apptainers']['sequana_tools']
    shell:
        "bamtools stats -in {input} > {output}"


rule mapping_index:
        input: "{sample}/mapping/mapping.sorted.bam"
        output: "{sample}/mapping/mapping.sorted.bam.bai"
        threads:
            1
        container:
            config['apptainers']['sequana_tools']
        shell:
            "bamtools index -in {input}"


if do_freebayes:
    __freebayes__output = "{sample}/freebayes/variants.raw.vcf"

    rule freebayes:
        input:
            bam ="{sample}/mapping/mapping.sorted.bam",
            ref= reference
        output:
            __freebayes__output
        log:
            "{sample}/freebayes/{sample}_freebayes.log"
        params:
            ploidy=config["freebayes"]["ploidy"],
            options=config["freebayes"]["options"]
        container:
            config['apptainers']['sequana_tools']
        wrapper:
            f"{manager.wrappers}/wrappers/freebayes"

if do_split_multiallelic:

    __split_multiallelic__output = "{sample}/freebayes/variants.raw.split.vcf"
    rule split_multiallelic:
        input:
            "{sample}/freebayes/variants.raw.vcf"
        output:
            "{sample}/freebayes/variants.raw.split.vcf"
        container:
            config['apptainers']['sequana_tools']
        shell:
            """
            bcftools norm -m -both {input} | vt decompose_blocksub - > {output}
            """
    __freebayes__output = __split_multiallelic__output



if do_freebayes and do_snpeff:
    #__snpeff__output = "{sample}/snpeff/variants.ann.vcf"
    #__snpeff__html =   "{sample}/snpeff/snpeff.html"
    #__snpeff__log =    "{sample}/snpeff/snpeff.log"
    # HERE is it important to include the sample in snpeff.csv file so that
    # multiqc can differentiate the files from each other.
    #__snpeff__csv =    "{sample}/snpeff/{sample}.snpeff.csv"

    rule snpeff:
        input:
            #vcf = "{sample}/freebayes/{sample}.raw.vcf",
            vcf = __freebayes__output,
            ann = annotation_file
        output:
            html="{sample}/snpeff/{sample}.snpeff.html",
            csv="{sample}/snpeff/{sample}.snpeff.csv",
            vcf="{sample}/snpeff/{sample}.ann.vcf"
        log:
            "{sample}/snpeff/{sample}_snpeff.log"
        params:
            options=config["snpeff"]["options"]
        container:
            config['apptainers']['sequana_tools']
        wrapper:
            f"{manager.wrappers}/wrappers/snpeff"

    __freebayes_vcf_filter__input = "{sample}/snpeff/{sample}.ann.vcf"
elif do_freebayes:
    __freebayes_vcf_filter__input = __freebayes__output


if do_freebayes and do_freebayes_vcf_filter:
    # __freebayes_vcf_filter__input defined above in the do_snpeff switch
    #__freebayes_vcf_filter__output = "{sample}/freebayes/variants.filtered.vcf"
    #__freebayes_vcf_filter__csv = "{sample}/freebayes/variants.filtered.csv"
    #__freebayes_vcf_filter__report_dir = "{sample}/report_variant"
    #__freebayes_vcf_filter__html =  "{sample}/report_variant/variant_calling.html"

    rule freebayes_vcf_filter:
        input:
            __freebayes_vcf_filter__input
        output:
            vcf="{sample}/freebayes_vcf_filter/{sample}.filter.vcf",
            csv="{sample}/freebayes_vcf_filter/{sample}.filter.csv",
            html="{sample}/report_variant/variant_calling.html"
        params:
            filter_dict=config["freebayes_vcf_filter"]
        container:
            config['apptainers']['sequana_tools']
        wrapper:
            f"{manager.wrappers}/wrappers/freebayes_vcf_filter"


rule igv_bases:
    # an alternative but that gives only the consensus
    # samtools mpileup -R -B -f ../../reference/CCR5_whole_locus.fasta
    # mapping.sorted.bam > out
    #
    input:
        bam="{sample}/mapping/mapping.sorted.bam",
        bai="{sample}/mapping/mapping.sorted.bam.bai"
    output:
        "{sample}/igvtools/bases.txt"
    params:
        reference = reference
    log:
        "{sample}/igvtools/coverage.log"
    container:
        config['apptainers']['sequana_tools']
    shell:
        """
        igvtools count {input.bam} stdout {params.reference} -w 1 --bases 1> {output} 2>{log}
        """


rule igv_count:
    input:
        bam="{sample}/mapping/mapping.sorted.bam",
        bai="{sample}/mapping/mapping.sorted.bam.bai"
    output:
        "{sample}/igvtools/coverage.txt"
    params:
        reference = reference
    log:
        "{sample}/igvtools/coverage.log"
    container:
        config['apptainers']['sequana_tools']
    shell:
        """
        igvtools count {input.bam} stdout {params.reference} -w 1 > {output} 2>{log}
        """


rule coverage:
    input:
        "{sample}/igvtools/coverage.txt"
    output:
        "{sample}/images/coverage.png"
    run:
        from pylab import savefig, ylim, xlabel
        import pandas as pd
        try:df  = pd.read_csv(input[0], skiprows=2, sep="\t",  header=None)
        except:df  = pd.read_csv(input[0], skiprows=4, sep="\t", header=None)
        df.plot(legend=False)
        ylim([0, max(ylim())])
        xlabel("position", fontsize=16)
        savefig(output[0], dpi=150)


# ========================================================== rulegraph
rule rulegraph:
    input: str(manager.snakefile)
    output:
        svg = "rulegraph/rulegraph.dot"
    params:
        mapper = {"multiqc": "../multiqc/multiqc_report.html"},
        configname = "config.yaml"
    wrapper:
        f"{manager.wrappers}/wrappers/rulegraph"


rule dot2svg:
    input:
        "rulegraph/rulegraph.dot"
    output:
        ".sequana/rulegraph.svg"
    container:
        config['apptainers']['graphviz']
    shell:
        """dot -Tsvg {input} -o {output}"""




if do_snpeff:
    __multiqc__input = (
        expand("{sample}/snpeff/{sample}.snpeff.csv", sample=manager.samples),
        expand("{sample}/mapping/samtools_{sample}.txt", sample=manager.samples),
        expand("{sample}/mapping/sequana_bamtools_stats_{sample}.txt", sample=manager.samples),
        expand("{sample}/sequana_laa_{sample}.json", sample=manager.samples))
else:
    __multiqc__input = (
        expand("{sample}/mapping/samtools_{sample}.txt", sample=manager.samples),
        expand("{sample}/mapping/sequana_bamtools_stats_{sample}.txt", sample=manager.samples),
        expand("{sample}/sequana_laa_{sample}.json", sample=manager.samples))


modules = config['multiqc']['modules']
if do_kraken:
    modules += " sequana_kraken "
if do_snpeff:
    modules += " snpeff "

rule multiqc:
    input:
        __multiqc__input
    output:
       "multiqc/multiqc_report.html"
    params:
        options=config['multiqc']['options'],
        input_directory=config['multiqc']['input_directory'],
        config_file=config['multiqc']['config_file'],
        modules=modules
    log:
        "multiqc/multiqc.log"
    container:
        config["apptainers"]["multiqc"]
    wrapper:
       f"{manager.wrappers}/wrappers/multiqc"


if do_kraken:
    rule create_proportion_plot_kraken:
        input: expand("{sample}/taxonomy/kraken/kraken.csv", sample=sorted(manager.samples))
        output:
            image="images/proportion_kraken.png",
            data="outputs/sequana_kraken_summary.json"
        run:
            import json
            from sequana.kraken.multikraken import MultiKrakenResults

            k = MultiKrakenResults(input, sample_names=sorted(manager.samples))
            fontsize = 12
            if len(input) > 30:
                fontsize = 10
            elif len(input)> 50:
                fontsize = 8
            k.plot_stacked_hist(output.image, dpi=200, ytick_fontsize=fontsize)

            with open(output.data, "w") as fout:
                json.dump(k.get_df().to_dict(), fout, indent=True, sort_keys=True)

if do_freebayes:
    __consensus__input_bases = "{sample}/igvtools/bases.txt"
    __consensus__input_freebayes = "{sample}/freebayes/variants.raw.vcf"
else:
    __consensus__input_bases = "{sample}/igvtools/bases.txt"
    __consensus__input_freebayes = None


rule consensus:
    """


    If coverage is null, igvtools skip the rows. For instance, if pos 3 and 4 
    are not covered, you end up with this kind of results

    pos A  C G T 
    1   10 0 0 0
    2   10 0 0 0
    5   10 0 0 0
    6   10 0 0 0

    At position 3 and 4 there is no data to parse, so the consensus will not have those
    entries.

    Another issue is the mix of highly covered regions vs low covered regions.
    For instance if the coverage is about 1000 and there is a region covered by 1 read only (coverage of 1X), this could be a chimera or artefact. We therefore set a minimal depth coverage below whihc data is removed.

    pos A  C G T 
    1   0 0 0 1000
    2   0 0 0 1000
    3   0 0 0 5
    4   0 0 0 1000

    becomes

    pos A  C G T 
    1   0 0 0 1000
    2   0 0 0 1000
    4   0 0 0 1000

    At position 3 there is no data to parse, so the consensus will not have that position



    """
    input:
        bases = __consensus__input_bases,
        freebayes = __consensus__input_freebayes
    output:
        consensus = "{sample}/consensus/consensus.fa",
        population = "{sample}/consensus/population.csv"
    params:
        min_depth = 10
    run:
        from sequana.laa import Consensus
        c = Consensus(input.bases, input.freebayes)
        c.min_depth = params.min_depth

        # save the consensus including deletion if identified in the VCF file
        sample_name = input[0].split("/")[0]
        c.save_consensus(output.consensus, sample_name)

        # save list of SNPs with at least 2 different nucleotides
        selection = c.get_population(threshold=0.1, Npop=2)
        selection.to_csv(output.population)


rule mafft:
    """Aligning the sequences using MAFFT. 

    """
    input:
        reference= reference,
        fasta="outputs/allfasta.fa"
    output:
        aln="outputs/mafft/mafft.aln.fa"
    threads:
        2
    container:
        config['apptainers']['sequana_tools']
    shell:
        """
        mafft --thread {threads} --retree 1 --maxiterate 0 --add {input.fasta} \
        --keeplength {input.reference} > {output.aln}
        """


rule raxml_mltree:
    """Estimating trees with RAxML

    -p seed for parsimony
    -b turn on bootstrapping
    -N number of bootstrap

    First, we build 20 ML trees on distinct starting trees and also print 
    the best likelihood in bestTree.T1::

        raxmlHPC -m GTRGAMMA -p 12345 -N 20 -s aln.fa -n T1

    This will generate 20 trees T1.RUN.X X=1..20 and one bestree.T1

    reference: https://cme.h-its.org/exelixis/web/software/raxml/hands_on.html

    """
    input:
        aln = "outputs/mafft/mafft.aln.fa"
    output:
        tree = "outputs/raxml/RAxML_bestTree.T1111",
        log = "outputs/raxml/raxml_parcimony.log",
    threads: 4
    params:
        N=config['raxml_mltree']['N'],
        options=config['raxml_mltree']['options'],
        wkdir=os.path.abspath("outputs/raxml")
    container:
        config['apptainers']['raxml']
    shell:
        """
        # First, let us get a best likelihood tree from 20 ML trees
        mkdir -p outputs/raxml && \
        rm -f outputs/raxml/*T1111* &&\
        raxmlHPC-PTHREADS -m GTRGAMMA -s {input.aln}  -p 12345  -T {threads} -w {params.wkdir} -N {params.N} -n T1111 1>{output.log}
        """


rule raxml_bootstrap:
    """
    If we want to do bootstrapinp, we need to use the -b option (seed)::

        raxmlHPC -m GTRGAMMA -p 12345 -b 12345 -N 100 -s aln.fa -n T2

    the bootstrap replicate trees is printed to a file called RAxML_bootstrap.T2
    """


    input:
        aln = "outputs/mafft/mafft.aln.fa"
    output:
        tree = "outputs/raxml/RAxML_bootstrap.T2222"
    threads: 4
    params:
        N=config['raxml_bootstrap']['N'],
        options=config['raxml_bootstrap']['options'],
        wkdir=os.path.abspath("outputs/raxml")
    container:
        config['apptainers']['raxml']
    log:
        "outputs/raxml/raxml_bootstrap.log",
    shell:
        """
        mkdir -p outputs/raxml
        rm -f outputs/raxml/*T2222*

        raxmlHPC-PTHREADS -m GTRGAMMA -s {input.aln}  -p 12345  -b 12345 -T {threads} \
             -w {params.wkdir}  \
             -N {params.N} \
             -n T2222  1>{log}
        """


rule raxml_bipartitions:
    """we can now use them to draw bipartitions on the best ML tree as follows:

        raxmlHPC -m GTRCAT -p 12345 -f b -t RAxML_bestTree.T1 -z RAxML_bootstrap.T2 -n T3

    This call will produce to output files that can be visualized with Dendroscope:
    RAxML_bipartitions.T15 (support values assigned to nodes) and RAxML_bipartitionsBranchLabels.T15 (support values assigned to branches of the tree). Note that, for unrooted trees the correct representation is actually the one with support values assigned to branches and not nodes of the tree!

    """
    input:
        best="outputs/raxml/RAxML_bestTree.T1111",
        boot="outputs/raxml/RAxML_bootstrap.T2222"
    output:
        parti="outputs/raxml/RAxML_bipartitions.T3333"
    params:
        wkdir=os.path.abspath("outputs/raxml")
    threads: 1
    container:
        config['apptainers']['raxml']
    shell:
        """
        # no need for threading
        mkdir -p outputs/raxml
        rm -f outputs/raxml/*T3333*

        raxmlHPC -m GTRCAT -f b -p 12345 -w {params.wkdir} -t {input.best} -z {input.boot} -n T3333

        """

if config['itol']['do']:
    rule itol:
        """
            https://github.com/albertyw/itolapi

        """
        input:
            best="outputs/raxml/RAxML_bestTree.T1111",
            boot="outputs/raxml/RAxML_bootstrap.T2222"
        output:
            png="outputs/dendogram.png",
            svg="outputs/dendogram.svg",
            pdf="outputs/dendogram.pdf"
        run:
            shell("mkdir -p outputs/itol")
            shell("cp {} {}".format(input.best, "outputs/itol/tree_of_life.tree.txt"))

            from sequana import ITOL

            itol = ITOL("outputs/itol/tree_of_life.tree.txt")
            itol.params['treeName'] = "amplicon"
            itol.upload()

            itol.params['display_mode'] = 2
            itol.params['ignore_branch_length'] = 1
            itol.params['line_width'] = 5
            itol.params['bootstrap_display'] = 1
            itol.params['bootstrap_type'] = 4
            itol.params['bootstrap_label_size'] = 32

            itol.export(output.png)
            itol.export(output.svg)
            itol.export(output.pdf)


rule mapping_consensus:
    input: 
        reference=reference,
        consensus="outputs/allfasta.fa"
    output:
        sorted_bam = "outputs/mapping_consensus/mapping.sorted.bam",
        sorted_bam_index = "outputs/mapping_consensus/mapping.sorted.bam.bai",
        bam = temp("outputs/mapping_consensus/mapping.bam"),
        sam = temp("outputs/mapping_consensus/mapping.sam")
    threads: 4
    log: "outputs/mapping_consensus/mapping.log"
    container:
        config["apptainers"]["sequana_tools"]
    shell:
        """
        minimap2 -x map-pb {input.reference} {input.consensus} -t {threads} -a 1> {output.sam} 2>{log}
        samtools view -Sbh -@ {threads} {output.sam} > {output.bam}
        bamtools sort -in {output.bam} -out {output.sorted_bam}
        samtools index {output.sorted_bam}
        """


rule remapping:
    """Remapp the consensus genomes onto the reference"""

    input:
        reference="{sample}/consensus/consensus.fa",
        fastq=get_input_fastq
    output:
        sorted_bam = "{sample}/remapping/mapping.sorted.bam",
        sorted_bam_index = "{sample}/remapping/mapping.sorted.bam.bai",
        bam = temp("{sample}/remapping/mapping.bam"),
        sam = temp("{sample}/remapping/mapping.sam")
    threads: 4
    log:
        "{sample}/mapping/mapping.log"
    container:
        config['apptainers']['sequana_tools']
    shell:
        """
        minimap2 -x map-pb {input.reference} {input.fastq} -t {threads} -a 1> {output.sam} 2>{log}
        samtools view -Sbh -@ {threads} {output.sam} > {output.bam}
        bamtools sort -in {output.bam} -out {output.sorted_bam}
        samtools index  {output.sorted_bam}
        """


rule build_fasta:
    input: expand("{sample}/consensus/consensus.fa", sample=sorted(manager.samples))
    output: "outputs/allfasta.fa"
    shell: "cat {input} > {output}"


if do_kraken:
    rule sequana_kraken:
        #input: lambda wildcards: manager.samples[wildcards.sample]
        input: get_input_fastq
        output:
            html="{sample}/taxonomy/summary.html",
            csv= "{sample}/taxonomy/kraken/kraken.csv"
        threads: 4
        run:
            #print(input)
            outdir = output.html.split("/",1)[0]
            cmd = "sequana_taxonomy --file1 {} "
            cmd += " --output-directory {}/taxonomy "
            cmd += " --thread {} --databases "
            cmd = cmd.format(input[0], outdir, threads)
            for this in config['kraken']['databases']:
                assert os.path.exists(this), "databases {} does not exits".format(this)
                cmd += " {} ".format(this)
            shell(cmd)
    html="{sample}/taxonomy/summary.html",


rule plot_ccs_histo:
    input: manager.ff.realpaths
    output: "images/plot_ccs_histo.png"
    run:
        from sequana import FastQ
        data = [len(FastQ(filename)) for filename in input]
        from pylab import plot, hist, savefig, xlabel
        hist(data, bins=10, lw=1, edgecolor="k")
        xlabel("Number of reads (in CCS bam file) per bar code", fontsize=16)
        savefig(output[0], dpi=150)


# TOFIX
do_smrtlink_report = False
if do_smrtlink_report:
    rule barcoding:
        input: __copy_smrtlink__output
        output:
            "images/barcoding_subreads_histogram.png",
            "images/barcoding_hist_polymerase_per_barcode.png",
            "images/barcoding_hist_quality_per_barcode.png",
            "images/barcoding_hist_mean_polymerase_read_length.png",
            "images/barcoding_polymerase_per_barcode.png"
        run:
            from sequana.pacbio import Barcoding
            bc = Barcoding(input[0])
            import pylab

            def savefile(filename):
                pylab.savefig("images/" + filename, dpi=200)

            pylab.clf()
            bc.hist_polymerase_per_barcode()
            savefile("barcoding_hist_polymerase_per_barcode.png")

            pylab.clf()
            bc.hist_quality_per_barcode()
            savefile("barcoding_hist_quality_per_barcode.png")

            pylab.clf()
            bc.hist_mean_polymerase_read_length()
            savefile("barcoding_hist_mean_polymerase_read_length.png")

            pylab.clf()
            bc.plot_polymerase_per_barcode(unbarcoded=False)
            savefile("barcoding_polymerase_per_barcode.png")

            pylab.clf()
            bc.plot_subreads_histogram()
            savefile("barcoding_subreads_histogram.png")


if do_snpeff:
    if config['itol']['do']:
        localrules: copy_genbank, copy_reference, itol
    else:
        localrules: copy_genbank, copy_reference
else:
    if config['itol']['do']:
        localrules: copy_reference, itol
    else:
        localrules: copy_reference


onsuccess:

    from sequana.modules_report.summary import SequanaReport
    from sequana.modules_report.kraken import KrakenModule

    manager.teardown(extra_files_to_remove=['igv.log'])

    intro = """
      <h2>Long Read Amplicon Analysis Summary.</h2> <br>
      <br><b>Number of samples:</b> {} </br>
      <b> MultiQC report: </b> <a href="multiqc/multiqc_report.html">multiqc report.</a> This multiqc report contains comparative analysis for the different analysis performed by the pipeline including the snpeff, raw reads information (e.g. number of CCS reads in each barcode), taxonomic analysis, variant calling (using freebayes) and some mapping statistics. 

Here below you can find all individual links to HTML reports for each sample for snpeff, kraken and freebayes analysis.



 <h2>Directory Tree </h2><p>

  Each barcode (sample) is analysed and the corresponding results are stored in a 
  tree structure similar to this one (summary):

 <a href=".">.</a><br>
 ├── <a href="./25/">25</a><br>
 │   ├── <a href="./25/consensus/">consensus</a><br>
 │   │   └── <a href="./25/consensus/consensus.fa">consensus.fa</a><br>
 │   │   └── <a href="./25/consensus/consensus.fa">population.csv</a><br>
 │   ├── <a href="./25/freebayes/">freebayes</a><br>
 │   │   ├── <a href="./25/freebayes/variants.filtered.vcf">variants.filtered.vcf</a><br>
 │   │   └── <a href="./25/freebayes/variants.raw.vcf">variants.raw.vcf</a><br>
 │   ├── <a href="./25/images/">images</a><br>
 │   │   └── <a href="./25/images/coverage.png">coverage.png</a><br>
 │   ├── <a href="./25/mapping/">mapping</a><br>
 │   │   ├── <a href="./25/mapping/mapping.sorted.bam">mapping.sorted.bam</a><br>
 │   │   ├── <a href="./25/mapping/mapping.sorted.bam.bai">mapping.sorted.bam.bai</a><br>
 │   │   ├── <a href="./25/mapping/samtools_25.txt">samtools_25.txt</a><br>
 │   │   └── <a href="./25/mapping/sequana_bamtools_stats_25.txt">sequana_bamtools_stats_25.txt</a><br>
 │   ├── <a href="./25/report_variant/">report_variant</a><br>
 │   │   └── <a href="./25/report_variant/variant_calling.html">variant_calling.html</a><br>
 │   ├── <a href="./25/remapping/">remapping</a><br>
 │   │   ├── <a href="./25/remapping/mapping.sorted.bam">mapping.sorted.bam</a><br>
 │   │   ├── <a href="./25/remapping/mapping.sorted.bam.bai">mapping.sorted.bam.bai</a><br>
 │   ├── <a href="./25/snpeff/">snpeff</a><br>
 │   │   ├── <a href="./25/snpeff/25.snpeff.csv">25.snpeff.csv</a><br>
 │   │   ├── <a href="./25/snpeff/25.snpeff.genes.txt">25.snpeff.genes.txt</a><br>
 │   │   ├── <a href="./25/snpeff/snpeff.html">snpeff.html</a><br>
 │   │   └── <a href="./25/snpeff/variants.ann.vcf">variants.ann.vcf</a><br>
 │   └── <a href="./25/taxonomy/">taxonomy</a><br>
 │   &nbsp;&nbsp;&nbsp; └── <a href="./25/taxonomy/summary.html">summary.html</a><br>
 <br><br>

<p>
<ul>

<li>The consensus directory contains a consensus.fa file created as follows. Based on the mapping.sorted.bam file to be found in ./mapping, we extract the nucleotides proportions (including deletions and insertions) -- using igvtool -- and keep at each position the nucleotide that is most present. If it is a deletion, it is ignored. Be aware that if a nucleotide is represented by two nucleotides is the same proportion (e.g.; 49/51%) then only one is kept (the one present in 51% of the case). Similarly, if there are 3 or 4 populations leading to a nucleotide supported by e.g. only 30% of the reads. To keep track of this information, we create a second file called <i>population.csv</i>. It contains all positions where there are at least 2 nuleotides supported by 10% of the reads.
</li>

<li>
In the freebayes directory, we store the variants in VCF format, which were created by freebayes. We keep the raw data with all variants including non significant calls. We then stored a filtered versio in <i>variants.filtered.vcf</i>.
</li>

<li>The images directory contains an image showing the coverage along the reference. </li>

<li>The mapping directory contains the mapped reads on the reference. It is a BAM file format and can be visualised in tools such as IGV. It contains the indexed file required by IGV. The reference is stored in the main directoy (see later) In this directory, we also store some files with statistics on the BAM file.
</li>

<li>The report_variant directory contains a convenient HTML report for variant, which is produced with sequana tools. It uses the filtered VCF file as input, which can be furthered introspect.
</li>

<li>The <i>remapping</i> directory contains the mapped reads in BAM format on the consensus.</li>

<li>The report_variant directory contains a convenient HTML report for variant, which is produced with sequana tools. It uses the filtered VCF file as input, which can be furthered introspect.

<li>
The snpeff directory contains snpeff results and its HTML report.
</li>

<li>
The taxonomy directory contains results about the taxonomic analysis based on kraken and sequana report.
</li>


</ul>

<p>



</p>

 <h2>Common directory Tree </h2><p>

The common directory contains final results or files required to redo the analysis.


<p>
 ├── <a href="./multiqc/multiqc_report.html">multiqc_report.html</a><br>
 ├── <a href="./outputs/">outputs</a><br>
 │   ├── <a href="./outputs/allfasta.fa">allfasta.fa</a><br>
 │   ├── <a href="./outputs/dendogram.png">dendogram</a><br>
 │   ├── <a href="./outputs/raxml/">raxml </a><br>
 │   ├── <a href="./outputs/itol/">itol</a><br>
 │   ├── <a href="./outputs/sequana_kraken_summary.json">sequana_kraken_summary.json</a><br>
 ├── <a href="./reference/">reference</a><br>
 │   ├── <a href="./reference/AB038249.fa">AB038249.fa</a><br>
 │   └── <a href="./reference/AB038249.gbk">AB038249.gbk</a><br>
 ├── <a href="./summary.html">summary.html</a><br>
</p>
<p>At the root directory level you can find a multiqc report in HTML format, a directory called outputs that contains the concatentation of all consensus genome together (allfasta.fa) with a phylogenetic analyses stored in raxml firectory. The reference used is stored in ./reference with the genbank if provided. Finally, you can find this summary HTML page (summary.html).</p>


      </p>
        <h2>Statistics</h2>
        <p>Number of CCS reads per barcode</p>
      <img src="images/plot_ccs_histo.png"></img>
        """

    if do_smrtlink_report:
        intro +="""
            <div style="width=50%">
                <img src="images/barcoding_subreads_histogram.png">
                <img src="images/barcoding_hist_polymerase_per_barcode.png">
                <img src="images/barcoding_hist_quality_per_barcode.png">
                <img src="images/barcoding_hist_mean_polymerase_read_length.png">
                <img src="images/barcoding_polymerase_per_barcode.png">
            </div>
        """

    if do_kraken:
        intro += """<h2>Kraken analysis</h2>
      <p>Proportion of reads in virus, bacteria, human and unclassified categories:</p>
      <img src="images/proportion_kraken.png"></img><br>
        """
        for sample in sorted(manager.samples):
            intro += """<a href="{}/taxonomy/summary.html">{}/kraken</a><br>""".format(sample, sample)


    if do_snpeff:
        intro += """<div><h2>SNP annotations</h2><p>Here below are links to
SNPEff reports and variant calling reports. For a snpeff summary, please see
this <a href="multiqc_report.html">multiqc report.</a></p>
        """
        for sample in sorted(manager.samples):
            intro += """<a href="{}/snpeff/snpeff.html">{}/snpeff.html</a><br>""".format(sample, sample)
        intro += "</div>"

    else:
        intro +="""<p>No SNPEFF report available (was not required). Please see variant reports instead</p></div>"""

    intro += "<h2>Variant reports</h2>"
    for sample in sorted(manager.samples):
        intro += """<a href="{}/report_variant/variant_calling.html">{}/variant report</a><br>""".format(sample, sample)

    try:intro += "<h2>Phylogeny</h2><div><img src={}></img></div>".format(__phylogeny__output)
    except NameError:pass


    intro = intro.format(len(manager.ff))


    data = manager.getmetadata()

    s = SequanaReport(data, intro=intro)

    if do_kraken:
        from sequana.utils import config as conf
        from pathlib import Path
        import json

        for sample in sorted(manager.samples):
            sample_summary = {}

            conf.summary_sections = []
            conf.output_dir = "{}".format(sample)

            k = KrakenModule( Path(sample) / "taxonomy"  / "kraken")
            sample_summary['kraken_json'] = json.loads(k._get_stats().to_json())
            conf.summary_sections.append({
                "name": "Kraken ",
                "anchor'": "kraken",
                "content": k._get_summary_section()
            })


    shell("chmod -R g+w .")
    shell("rm -rf rulegraph")

onerror:
    manager.onerror()

